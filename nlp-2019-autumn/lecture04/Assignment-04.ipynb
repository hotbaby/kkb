{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 04 Deep learning and backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 复习上课内容以及复现课程代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考`Lecture-04.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 理论题目"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does a nueron compute?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "神经元的计算包括线性变换和非线性变化两部分。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "线性变换：$z = wx + b$\n",
    "\n",
    "非线性变换： $a = \\sigma (z)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why we use non-linear activation functions in neural networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果没有非线性变换，深层的神经网络的效果以单层的神经网络的效果一样。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\hat{y} = W^{[1]}*W^{[2]} ... W^{[n]}*X + b = W*X + b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's is the Logistic Loss?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "逻辑回归通过`sigmoid`的函数把$(-\\infty, +\\infty)$的数映射到$(0, 1)$区间上。逻辑回归常用二分类问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\sigma(z) = \\frac{1}{1+e^{-z}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assume that you are building a binary classifier for dectecting if an image containing cats, which activation function would you recommend using for the output layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A. ReLU    \n",
    "B. Leaky ReLU    \n",
    "C. sigmoid    \n",
    "D. tanh  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why we don't use zero initialization for all parameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一层多个神经元的效果与一个神经元的效果是一样的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can you implemented the softmax function using Python?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    X = X - np.max(X)\n",
    "    return np.exp(X) / np.sum(np.exp(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax([1, 3, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实践题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a simple digits recognizer to check if the digit in the image is lager than 5?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Package Dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "digist_dataset = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1797, 64), (1797,))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digist_dataset.data.shape, digist_dataset.target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(1,11):\n",
    "    plt.subplot(2,5,i)\n",
    "    plt.imshow(digist_dataset.data[i-1].reshape([8,8]),cmap=plt.cm.gray_r)\n",
    "    plt.text(3,10,str(digist_dataset.target[i-1]))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into training and test set\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(digist_dataset.data, digist_dataset.target, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1347, 64), (450, 64), (1347,), (450,))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, Y_train.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reform the label.\n",
    "# If the digit is smaller than 5, the label is 0.\n",
    "# If the digit is larger than 5, the label is 1.\n",
    "\n",
    "y_train[y_train < 5] = 0\n",
    "y_train[y_train >= 5] = 1\n",
    "y_test[y_test < 5] = 0\n",
    "y_test[y_test >= 5] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Architecture of Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mathematical expression of the algorithm**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$z^{(i)} = W^T  * x^{(i)} + b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\hat y^{(i)}=a^{(i)} = \\sigma (z^{(i)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$L(y_i, \\hat{y_i}) = -y_i*\\log{\\hat y_i} - (1-y_i)\\log(1-\\hat{y_i})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The total cost over all training examples**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$J = \\frac{1}{m} \\sum{L(y_i, \\hat{y_i})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the alogrithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Activation function**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\sigma(x) = \\frac{1}{1+e^{-x}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x:list):\n",
    "    _sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
    "    return list(map(_sigmoid, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid(np.array([-1, 0, 1, 2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Initialize Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random innitialize the parameters\n",
    "\n",
    "def initialize_parameters(dim):\n",
    "    '''\n",
    "    Argument: dim -- size of the w vector\n",
    "    \n",
    "    Returns:\n",
    "    w -- initialized vector of shape (dim,1)\n",
    "    b -- initializaed scalar\n",
    "    '''\n",
    "    \n",
    "    w = np.random.randn(dim, 1)\n",
    "    b = np.random.random()\n",
    "    \n",
    "    assert(w.shape == (dim,1))\n",
    "    assert(isinstance(b, float) or isinstance(b, int))\n",
    "    \n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize_parameters(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Forward and Backward Propagation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward Propagation:   \n",
    ". X    \n",
    ". A = $\\sigma(w^T*X+b) = (a^{(1)},a^{(2)},...,a^{(m)})$   \n",
    ". J = $-\\frac{1}{m} \\sum_{i=1}^{m}y^{(i)}log(a^{(i)}+(1-y^{(i)})log(1-a^{(i)})$       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some derivative: \n",
    "$$\\frac{\\partial{J}}{\\partial{w}} = \\frac{1}{m}X*(A-Y)^T$$   \n",
    "$$\\frac{\\partial{J}}{\\partial{b}} = \\frac{1}{m}\\sum_{i=1}^m(a^{(i)}-y^{(i)}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$J(y_i, \\hat{y_i}) = -\\frac{1}{m}\\sum[y_i*\\log{\\hat y_i} + (1-y_i)\\log(1-\\hat{y_i})]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_cross_entropy(y, y_hat):\n",
    "    \"\"\"\n",
    "    Cross entropy\n",
    "    \"\"\"\n",
    "    y = np.array(y)\n",
    "    y_hat = np.array(y_hat)\n",
    "    assert y.shape == y_hat.shape\n",
    "           \n",
    "    return -np.sum([yi*np.log(yi_hat) + (1-yi)*np.log(1-yi_hat) for yi, yi_hat in zip(y, y_hat)])/y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost([1, 1, 0], [.99, .99, .01]), cross_entropy([1, 0, 1], [.99, 0.99, 0.88])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def propagate(w, b, X, Y):\n",
    "    '''\n",
    "    Forward and backward propagation.\n",
    "    \n",
    "    Arguments:\n",
    "    w - weights\n",
    "    b - bias\n",
    "    X - data\n",
    "    Y - ground truth\n",
    "    '''\n",
    "    def forward_propagation(W, b, X):\n",
    "        \"\"\"\n",
    "        Forward Propagaion.\n",
    "        \"\"\"\n",
    "        return sigmoid(np.dot(W, X) + b)\n",
    "\n",
    "    def backward_propagation(A, Y):\n",
    "        \"\"\"\n",
    "        Backward Propagation.\n",
    "\n",
    "        Param A: linear transformation ouput, shape (sample_num, feature_num)\n",
    "        \"\"\"\n",
    "        partial_w = np.dot(X, (A - Y).T) / X.shape[0]\n",
    "        partial_b = np.sum([A - Y]) / A.shape[0]\n",
    "\n",
    "        return partial_w, partial_b\n",
    "\n",
    "\n",
    "    m = X.shape[1]\n",
    "\n",
    "    # forward propagation.\n",
    "    A = forward_propagation(w, b, X)\n",
    "    cost = cost_cross_entropy(Y, A)\n",
    "\n",
    "    # backward propagation.\n",
    "    dw, db = backward_propagation(A, Y)\n",
    "    \n",
    "    assert(dw.shape == w.shape)\n",
    "    assert(db.dtype == float)\n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "\n",
    "    # backward TODO\n",
    "\n",
    "    grads = {'dw':dw,\n",
    "             'db':db}\n",
    "\n",
    "    return grads, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Optimization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimizing the cost function using gradient descent.\n",
    "$$ \\theta = \\theta - \\alpha * d\\theta$$\n",
    "where the $\\alpha$ is the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost=False):\n",
    "    '''\n",
    "    This function optimize w and b by running a gradient descen algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    w - weights\n",
    "    b - bias\n",
    "    X - data\n",
    "    Y - ground truth\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- True to print the loss every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    params - dictionary containing the weights w and bias b\n",
    "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
    "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        grads, cost = None\n",
    "        \n",
    "        dw = grads['dw']\n",
    "        db = grads['db']\n",
    "        \n",
    "        w = None\n",
    "        b = None\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    \n",
    "    params = {\"w\":w,\n",
    "              \"b\":b}\n",
    "    \n",
    "    grads = {\"dw\":dw,\n",
    "             \"db\":db}\n",
    "    \n",
    "    return params, grads, costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Predict**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two steps to finish this task:   \n",
    "1. Calculate $\\hat{Y} = A = \\sigma(w^T*X+b)$   \n",
    "2. Convert the entries of a into 0 (if activation <= 0.5) or 1 (if activation > 0.5), stores the predictions in a vector Y_prediction. If you wish, you can use an if/else statement in a for loop (though there is also a way to vectorize this)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w, b, X):\n",
    "    '''\n",
    "    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights\n",
    "    b -- bias \n",
    "    X -- data \n",
    "    \n",
    "    Returns:\n",
    "    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
    "    '''\n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1,m))\n",
    "    w = w.reshape(X.shape[0],1)\n",
    "    \n",
    "    A = None\n",
    "    \n",
    "    for i in range(A.shape[i]):\n",
    "        None \n",
    "    \n",
    "    assert(Y_prediction.shape == (1,m))\n",
    "    \n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Merge functions into a model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_trein, X_test, Y_test, num_iterations, learning_rate,print_cost):\n",
    "    \"\"\"\n",
    "    Build the logistic regression model by calling all the functions you have implemented.\n",
    "    Arguments:\n",
    "    X_train - training set\n",
    "    Y_train - training label\n",
    "    X_test - test set\n",
    "    Y_test - test label\n",
    "    num_iteration - hyperparameter representing the number of iterations to optimize the parameters\n",
    "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
    "    print_cost -- Set to true to print the cost every 100 iterations\n",
    "    \n",
    "    Returns:\n",
    "    d - dictionary should contain following information w,b,training_accuracy, test_accuracy,cost\n",
    "    eg: d = {\"w\":w,\n",
    "             \"b\":b,\n",
    "             \"training_accuracy\": traing_accuracy,\n",
    "             \"test_accuracy\":test_accuracy,\n",
    "             \"cost\":cost}\n",
    "    \"\"\"\n",
    "    None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 选做题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observe the effect of learning rate on the learning process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tips: plot the learning curve with different learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observe the effect of iteration_num on the test accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try use softmax function to build a model to recognize which digits is the image?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
