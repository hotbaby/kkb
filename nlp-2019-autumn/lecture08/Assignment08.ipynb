{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.复习课上内容， 阅读相应论文。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 回答以下理论题目"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.  What is autoencoder?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autoencoder是一种无监督的学习方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "网络结构，input -> Encoder NN -> Code Vector -> Decoder NN -> output(重建input)，input与output越相似越好，这样我们就可以用Encoder NN输出的Vector表征input，其中Code Vector的维度远远小于input的维度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autoencoder可以数据压缩，重建等任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. What are the differences between greedy search and beam search?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Greedy Search 每次输出最大概率的结果。\n",
    "* Beam Search 每次输出多个最有可能的结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seq2seq 是一个条件语言模型$P(Y|X)$，训练是的整个序列概率最大化。每次取最大概率的输出，整个序列概率的乘积不一定是最大概率。而Beam Search每次输出K个序列最大概率作为输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. What is the intuition of attention mechanism?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoder时，分别计算输入序列的相似度，在进行softmax后，分配不同的权重。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. What is the disadvantage of word embeding introduced in previous lectures ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不能解决一词多义的问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. What is the architecture of ELMo model. (A brief description is enough)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ELMo先训练一个双层双向的LSTM网络，再取出隐层向量，与输入层向量，乘以不同的权重求和，形成一个的表征向量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Compared to RNN,  what is the advantage of Transformer ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以并行计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Why we use layer normalizaiton instead of batch normalization in Transformer ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch normlization是在batch方向标准化，即不同输入同一个维度标准化。layer normalization是在同一个输入不同维度标准化。在Transformer（或其他RNN）中，一个输入通常是一个句子，一个batch由多个句子句子组成，一个batch不同输入的维度基本没有任何关系，因此在Transformer中使用layer normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Why we need position embedding in Transformer ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与RNN不同，Transformer在做attention时没有位置信息，第一个和最后一个位置做attention结果是一样的，因此要加上token在序列中相对和绝对位置信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Briefly describe what is self-attention and what is multi-head attention?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "self-attention: $Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{k_k}})V$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输入的Emebedding分别乘以不同的W，变成q、k、v，q表示query，k表示key，v表示value，q分别与k做attention，再经过softmax变成概率分布，输出权重$\\alpha$，$\\alpha$乘以缩放因子$\\frac{1}{\\sqrt d_k}$，再乘以V，得到self-attention embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "multi-head attention: $MultiHead(Q, K, V) = Concat(head_1, ..., head_k)W^O where head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-head attention在self-attention上进化而来，q，k，v变成multi head，在分别坐self-attention，再concat起来，再乘以W，输出embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. What is the basic unit of GPT model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Masked Multi Self Attention -> layer normalization -> Feed Forward -> Layer Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11. Briefly descibe how to use GPT in other NLP tasks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input -> GPT -> Linear -> ouput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12. What is masked language model in BERT ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLM将输入的Tokens随机标记为mask，然后预测masked的Token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13. What are the inputs of BERT ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input embedding是token embemdding, segment embedding和position embedding的和。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 14. Briely descibe how to use BERT in other NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT预训练生成单词的embedding, 再fine-tune其他的具体任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Sentence Pair Classification Tasks，[CLS]输出的embedding再去做分类\n",
    "2. Single Sentence Classification Tasks,  [CLS]输出embedding再去分类\n",
    "3. Question Answering Tasks: 两个vector分别与输出的embedding做分类，生成Start和End\n",
    "4. Single Sentence Tagging Tasks: token输出的embedding再去分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15. What are the differences between these three models: GPT, BERT, GPT2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT取得Transformer的Decoder结构，是单向的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT用的是Transformer的Encoder结构，是双向。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT2比GPT更复杂，更深，参数更多。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
